--- MemoryMate-PhotoFlow-Refactored-main/main_window_qt.py
+++ MemoryMate-PhotoFlow-Refactored-main/main_window_qt.py
@@ -106,6 +106,7 @@
 from ui.widgets.backfill_indicator import CompactBackfillIndicator
 from ui.widgets.selection_toolbar import SelectionToolbar
 from ui.activity_center import ActivityCenter
+from ui.metadata_editor_dock import MetadataEditorDock
 from ui.ui_builder import UIBuilder
 
 # Phase 2 Refactoring: Extracted services
@@ -1119,6 +1120,20 @@
         except Exception as e:
             print(f"[MainWindow] Could not create activity center: {e}")
             self.activity_center = None
+
+        # --- Metadata Editor Dock (QDockWidget, right-side, Lightroom-style info panel)
+        try:
+            self.metadata_editor_dock = MetadataEditorDock(self)
+            self.addDockWidget(Qt.DockWidgetArea.RightDockWidgetArea,
+                               self.metadata_editor_dock)
+            # Connect metadata changes to refresh UI if needed
+            self.metadata_editor_dock.metadataChanged.connect(
+                self._on_metadata_changed)
+            # Start hidden; toggle via View menu or toolbar button
+            self.metadata_editor_dock.hide()
+        except Exception as e:
+            print(f"[MainWindow] Could not create metadata editor dock: {e}")
+            self.metadata_editor_dock = None
 
         # --- Wire toolbar actions
         act_select_all.triggered.connect(self.grid.list_view.selectAll)
@@ -1243,52 +1258,221 @@
             import traceback
             traceback.print_exc()
 
-        # === Initialize database schema at startup ===
-        try:
-            from repository.base_repository import DatabaseConnection
-            db_conn = DatabaseConnection("reference_data.db", auto_init=True)
-            print("[Startup] Database schema initialized successfully")
-        except Exception as e:
-            print(f"[Startup] âš ï¸ Database initialization failed: {e}")
-            import traceback
-            traceback.print_exc()
+        # NOTE: Database schema initialization is now handled in splash_qt.py startup worker
+        # to avoid duplicate initialization. ReferenceDB() calls DatabaseConnection(auto_init=True)
+        # which handles schema creation and migrations.
 
         # CRITICAL FIX: Defer heavy initialization to avoid blocking UI thread
         # Schedule heavy operations to run after window is shown
-        QTimer.singleShot(100, self._deferred_initialization)
         
         # DIAGNOSTIC: Confirm __init__() is completing
         print("[MainWindow] âœ… âœ… âœ… __init__() COMPLETED - returning to main_qt.py")
         print(f"[MainWindow] Window object: {self}")
         print(f"[MainWindow] Window valid: {self.isValid() if hasattr(self, 'isValid') else 'N/A'}")
 
+    def showEvent(self, event):
+        """Guardrail: never start deferred init before the window is shown and has painted at least once."""
+        super().showEvent(event)
+        if getattr(self, "_startup_after_show_scheduled", False):
+            return
+        self._startup_after_show_scheduled = True
+
+        # Let Qt complete a paint cycle, then start deferred work.
+        from PySide6.QtCore import QTimer
+        QTimer.singleShot(0, self._after_first_paint)
+
+    def _after_first_paint(self):
+        """Runs right after the first paint, schedules heavy background work away from initial layout."""
+        from PySide6.QtCore import QTimer
+
+        if getattr(self, "_deferred_init_started", False):
+            return
+
+        # Guardrail 2: throttle shared background workers during initial layout stabilization.
+        try:
+            from services.job_manager import get_job_manager
+            jm = get_job_manager()
+            if hasattr(jm, "enable_startup_throttle"):
+                jm.enable_startup_throttle(max_threads=1)
+                # Restore after layout has likely stabilized and initial thumbnails are queued.
+                QTimer.singleShot(5000, jm.disable_startup_throttle)
+        except Exception:
+            # Throttle is best-effort, never block startup for it.
+            pass
+
+        # Guardrail 1: ensure the first render completes, then run deferred init.
+        QTimer.singleShot(250, self._deferred_initialization)
+
     def _deferred_initialization(self):
         """
         CRITICAL FIX: Perform heavy initialization operations after window is shown.
-        This prevents the UI from freezing during startup.
+
+        v9.3.0 FIX: Moved heavy DB operations (backfill, index optimization) to
+        background jobs. Only minimal DB handle creation happens in GUI thread.
+
+        This follows Material Design principle: App should be responsive immediately,
+        heavy work happens visibly in the background via Activity Center.
         """
         print("[MainWindow] Starting deferred initialization...")
-        
-        try:
-            # Initialize database and sidebar (was previously in __init__)
-            self._init_db_and_sidebar()
-            print("[MainWindow] âœ… Database and sidebar initialized")
-            
-            # Restore session state (was previously in __init__)
+
+        try:
+            # Step 1: Fast - create minimal DB handle (no heavy operations)
+            self._init_minimal_db_handle()
+            print("[MainWindow] âœ… Database handle initialized (fast)")
+
+            # Step 2: Restore session state
             QTimer.singleShot(300, self._restore_session_state)
             print("[MainWindow] âœ… Session state restoration scheduled")
-            
-            # Update status bar
+
+            # Step 3: Update status bar
             self._update_status_bar()
             print("[MainWindow] âœ… Status bar updated")
-            
+
+            # Step 4: Enqueue heavy DB maintenance as background job
+            # This runs visibly in Activity Center, doesn't block UI
+            # Step 4: Enqueue heavy DB maintenance as background job (delayed)
+            from PySide6.QtCore import QTimer
+            QTimer.singleShot(2000, self._enqueue_startup_maintenance_job)
+            print("[MainWindow] âœ… Database maintenance job enqueued")
+
+            # Step 5: Warmup CLIP model in background
+            # Step 5: Warmup CLIP model in background (delayed to protect first render)
+            from PySide6.QtCore import QTimer
+            QTimer.singleShot(15000, self._warmup_clip_in_background)
+            print("[MainWindow] âœ… CLIP warmup scheduled")
+
+            # Step 6 (FIX #6): Deferred thumbnail cache purge
+            # Moved here from splash_qt.py so startup isn't blocked.
+            # Step 6: Deferred thumbnail cache purge (delayed)
+            from PySide6.QtCore import QTimer
+            QTimer.singleShot(5000, self._deferred_cache_purge)
+            print("[MainWindow] âœ… Cache purge scheduled")
+
             print("[MainWindow] âœ… Deferred initialization completed successfully")
-            
+
         except Exception as e:
             print(f"[MainWindow] âš ï¸ Deferred initialization error: {e}")
             import traceback
             traceback.print_exc()
+
+    def _init_minimal_db_handle(self):
+        """
+        Fast DB initialization - only creates handle, no heavy operations.
+
+        Heavy operations (backfill, index optimization) are moved to
+        _enqueue_startup_maintenance_job() which runs in background.
+        """
+        from reference_db import ReferenceDB
+        self.db = ReferenceDB()
+
+        # Reload sidebar date tree (fast operation, uses cached data)
+        try:
+            if hasattr(self, 'sidebar') and hasattr(self.sidebar, 'reload_date_tree'):
+                self.sidebar.reload_date_tree()
+                print("[Sidebar] Date tree reloaded.")
+        except Exception as e:
+            print(f"[Sidebar] Failed to reload date tree: {e}")
+
+    def _enqueue_startup_maintenance_job(self):
+        """
+        Enqueue heavy DB maintenance as a tracked background job.
+
+        Uses the global JobManager singleton so the job always appears in the
+        Activity Center.  The worker thread gets its own ReferenceDB connection
+        (per-thread pool) and never touches Qt widgets.
+        """
+        import threading
+        try:
+            from services.job_manager import get_job_manager
+            jm = get_job_manager()
+
+            job_id = jm.register_tracked_job(
+                job_type="maintenance",
+                description="Database maintenance (backfill & index)",
+            )
+            print(f"[MainWindow] Maintenance job registered: job_id={job_id}")
+
+            def _maintenance():
+                try:
+                    from reference_db import ReferenceDB
+                    db = ReferenceDB()
+                    db.single_pass_backfill_created_fields()
+                    db.optimize_indexes()
+                    jm.complete_tracked_job(job_id, success=True)
+                    print("[MainWindow] Background maintenance completed")
+                except Exception as e:
+                    jm.complete_tracked_job(job_id, success=False, error=str(e))
+                    print(f"[MainWindow] Background maintenance failed: {e}")
+
+            thread = threading.Thread(target=_maintenance, name="startup_maintenance", daemon=True)
+            thread.start()
+
+        except Exception as e:
+            print(f"[MainWindow] Failed to enqueue maintenance job: {e}")
+            # Non-fatal â€” app can continue without optimization
+
+    def _warmup_clip_in_background(self):
+        """
+        Warm up CLIP model in a background thread after UI is shown.
+
+        This gives the best UX: UI appears immediately, and CLIP loads quietly
+        in the background while the user starts working. First semantic search
+        will be fast because model is already loaded.
+        """
+        try:
+            from settings_manager_qt import SettingsManager
+            settings = SettingsManager()
+
+            # Only warmup if semantic embeddings are enabled
+            if not settings.get("enable_semantic_embeddings", True):
+                print("[MainWindow] CLIP warmup skipped (semantic embeddings disabled)")
+                return
+
+            import threading
+
+            def _warmup():
+                try:
+                    from services.semantic_embedding_service import get_semantic_embedding_service
+                    svc = get_semantic_embedding_service()
+                    svc._load_model()  # Intentionally warm cache, tokenizer, weights
+                    print("[MainWindow] âœ… CLIP model warmed up in background")
+                except Exception as e:
+                    # Non-fatal - model will load on first use if warmup fails
+                    print(f"[MainWindow] âš ï¸ CLIP background warmup failed (non-fatal): {e}")
+
+            # Run in daemon thread so it doesn't block app shutdown
+            thread = threading.Thread(target=_warmup, name="clip_warmup", daemon=True)
+            thread.start()
+            print("[MainWindow] ðŸ§  CLIP background warmup started")
+
+        except Exception as e:
+            print(f"[MainWindow] âš ï¸ Could not start CLIP warmup: {e}")
     
+    def _deferred_cache_purge(self):
+        """FIX #6: Run thumbnail cache purge in a background thread after startup."""
+        try:
+            from settings_manager_qt import SettingsManager
+            settings = SettingsManager()
+            if not settings.get("cache_auto_cleanup", True):
+                print("[MainWindow] Cache auto-cleanup disabled, skipping purge")
+                return
+
+            import threading
+
+            def _purge():
+                try:
+                    from thumb_cache_db import get_cache
+                    cache = get_cache()
+                    cache.purge_stale(max_age_days=7)
+                    print("[MainWindow] Deferred cache purge completed")
+                except Exception as e:
+                    print(f"[MainWindow] Cache purge error (non-fatal): {e}")
+
+            thread = threading.Thread(target=_purge, name="cache_purge", daemon=True)
+            thread.start()
+        except Exception as e:
+            print(f"[MainWindow] Could not start cache purge: {e}")
+
     def ensureOnScreen(self):
         """
         CRITICAL FIX: Ensure window is positioned on a visible screen.
@@ -1330,45 +1514,9 @@
             print(f"[MainWindow] âœ“ Window is on-screen (center at {window_center_x}, {window_center_y})")
 
 
-# =========================
-    def _init_db_and_sidebar(self):
-        """
-        Initialize database schema, ensure created_* date fields, backfill if needed,
-        optimize indexes, and reload the sidebar date tree.
-
-        Runs on app startup to make sure the date navigation works immediately.
-        """
-        from reference_db import ReferenceDB
-        self.db = ReferenceDB()
-
-        # NOTE: Schema creation and migrations are now handled automatically
-        # by repository layer during ReferenceDB initialization.
-        # created_* columns are added via migration system (v1.5.0 migration).
-
-        # ðŸ•° Backfill if needed (populate data in existing columns)
-        try:
-            updated_rows = self.db.single_pass_backfill_created_fields()
-            if updated_rows:
-                print(f"[DB] Backfilled {updated_rows} legacy rows with created_* fields.")
-        except Exception as e:
-            print(f"[DB] Backfill failed (possibly empty DB): {e}")
-
-        # âš¡ Optimize indexes (important for large photo libraries)
-        try:
-            self.db.optimize_indexes()
-        except Exception as e:
-            print(f"[DB] optimize_indexes failed: {e}")
-
-        # ðŸŒ³ Reload sidebar date tree
-        try:
-            # Check if method exists before calling
-            if hasattr(self.sidebar, 'reload_date_tree'):
-                self.sidebar.reload_date_tree()
-                print("[Sidebar] Date tree reloaded.")
-            else:
-                print("[Sidebar] reload_date_tree() method not available - skipping")
-        except Exception as e:
-            print(f"[Sidebar] Failed to reload date tree: {e}")
+    # _init_db_and_sidebar() removed in v9.3.0 - replaced by:
+    # - _init_minimal_db_handle() for fast DB handle creation
+    # - _enqueue_startup_maintenance_job() for background heavy work
   
     # ============================================================
     # ðŸ·ï¸ Tag filter handler
@@ -1421,8 +1569,8 @@
     def _on_quick_search(self, query: str):
         """Handle quick search from search bar."""
         try:
-            from services import SearchService
-            search_service = SearchService()
+            from app_services import get_search_service
+            search_service = get_search_service()
             paths = search_service.quick_search(query, limit=100)
 
             # Display results in grid
@@ -1444,8 +1592,8 @@
             if dialog.exec() == QDialog.Accepted:
                 criteria = dialog.get_search_criteria()
 
-                from services import SearchService
-                search_service = SearchService()
+                from app_services import get_search_service
+                search_service = get_search_service()
                 result = search_service.search(criteria)
 
                 if result.paths:
@@ -2711,62 +2859,55 @@
 
     def _on_project_changed_by_id(self, project_id: int):
         """
-        Phase 2: Switch to a project by ID (used by breadcrumb navigation).
-        Updates the sidebar and grid to show the selected project.
+        Switch to a project by ID.
+
+        Delegates to the **active layout** via BaseLayout.set_project() so
+        that GooglePhotosLayout refreshes its AccordionSidebar while
+        CurrentLayout refreshes SidebarQt + grid.  MainWindow no longer
+        pokes hidden/dead widgets directly.
         """
         print(f"\n[MainWindow] ========== _on_project_changed_by_id({project_id}) STARTED ==========")
         try:
-            # CRITICAL: Check if already on this project to prevent redundant reloads and crashes
+            # Already on this project?
             current_project_id = getattr(self.grid, 'project_id', None) if hasattr(self, 'grid') else None
-            print(f"[MainWindow] Current project_id: {current_project_id}")
-            
             if current_project_id == project_id:
                 print(f"[MainWindow] Already on project {project_id}, skipping switch")
                 return
 
-            # PHASE 1: Save project_id to session state
+            # 1. Persist to session state
             from session_state_manager import get_session_state
             get_session_state().set_project(project_id)
-            print(f"[MainWindow] PHASE 1: Saved project_id={project_id} to session state")
-
-            print(f"[MainWindow] Step 1: Updating grid.project_id...")
-            # CRITICAL ORDER: Update grid FIRST before sidebar to prevent race condition
-            # Sidebar.set_project() triggers callbacks that reload grid, so grid.project_id
-            # must be set BEFORE those callbacks fire
-            if hasattr(self, "grid") and self.grid:
-                self.grid.project_id = project_id
-                print(f"[MainWindow] Step 1: âœ“ Set grid.project_id = {project_id}")
+
+            # 2. Delegate to the active layout (the layout owns its sidebar)
+            layout = None
+            if hasattr(self, 'layout_manager') and self.layout_manager:
+                layout = self.layout_manager.get_current_layout()
+
+            if layout is not None:
+                layout.set_project(project_id)
+                print(f"[MainWindow] Delegated to {type(layout).__name__}.set_project({project_id})")
             else:
-                print(f"[MainWindow] Step 1: âœ— Grid not available!")
-
-            print(f"[MainWindow] Step 2: Updating sidebar...")
-            # Now update sidebar (this triggers reload which will use the new grid.project_id)
-            if hasattr(self, "sidebar") and self.sidebar:
-                self.sidebar.set_project(project_id)
-                print(f"[MainWindow] Step 2: âœ“ Sidebar.set_project({project_id}) completed")
-            else:
-                print(f"[MainWindow] Step 2: âœ— Sidebar not available!")
-
-            print(f"[MainWindow] Step 3: Reloading grid to 'all' branch...")
-            # Finally, explicitly reload grid to show all photos
-            if hasattr(self, "grid") and self.grid:
-                self.grid.set_branch("all")  # Reset to show all photos
-                print(f"[MainWindow] Step 3: âœ“ Grid.set_branch('all') completed")
-            else:
-                print(f"[MainWindow] Step 3: âœ— Grid not available for set_branch!")
-
-            # CRITICAL FIX: Removed duplicate breadcrumb update!
-            # The gridReloaded signal (line 3392) already triggers _update_breadcrumb()
-            # Scheduling a second update here causes a race condition crash!
-            print(f"[MainWindow] Step 4: Breadcrumb will auto-update via gridReloaded signal")
-
-            print(f"[MainWindow] Step 5: âœ“âœ“âœ“ Switched to project ID: {project_id}")
+                # Fallback: no layout manager yet (very early startup)
+                if hasattr(self, "grid") and self.grid:
+                    self.grid.project_id = project_id
+                if hasattr(self, "sidebar") and self.sidebar:
+                    self.sidebar.set_project(project_id)
+
+            # 3. Reset grid branch to "all" for CurrentLayout's grid
+            #    (Google layout handles its own reload inside set_project)
+            layout_id = ""
+            if hasattr(self, 'layout_manager') and self.layout_manager:
+                layout_id = self.layout_manager.get_current_layout_id() or ""
+            if layout_id != "google":
+                if hasattr(self, "grid") and self.grid:
+                    self.grid.set_branch("all")
+
+            # Breadcrumb auto-updates via gridReloaded signal
             print(f"[MainWindow] ========== _on_project_changed_by_id({project_id}) COMPLETED ==========\n")
         except Exception as e:
-            print(f"[MainWindow] âœ—âœ—âœ— ERROR switching project: {e}")
+            print(f"[MainWindow] ERROR switching project: {e}")
             import traceback
             traceback.print_exc()
-            print(f"[MainWindow] ========== _on_project_changed_by_id({project_id}) FAILED ==========\n")
 
     def _refresh_project_list(self):
         """
@@ -3810,6 +3951,48 @@
         if act and act.isChecked() != visible:
             act.setChecked(visible)
 
+    # ------------------------------------------------------------------
+    # Metadata Editor Dock
+    # ------------------------------------------------------------------
+    def _toggle_metadata_editor(self, checked: bool = None):
+        """Toggle the Metadata Editor dock widget visibility."""
+        dock = getattr(self, "metadata_editor_dock", None)
+        if not dock:
+            return
+        if checked is None:
+            checked = not dock.isVisible()
+        dock.setVisible(checked)
+        # Sync the View menu checkbox
+        act = getattr(self, "_act_toggle_metadata_editor", None)
+        if act and act.isChecked() != checked:
+            act.setChecked(checked)
+
+    def _on_metadata_changed(self, photo_id: int, field: str, value):
+        """Handle metadata field changes from the dock editor."""
+        print(f"[MainWindow] Metadata changed: photo={photo_id}, {field}={value}")
+        # Refresh grid if rating/flag changed (may affect visual indicators)
+        if field in ("rating", "flag"):
+            grid = getattr(self, "grid", None)
+            if grid and hasattr(grid, "refresh_thumbnail"):
+                grid.refresh_thumbnail(photo_id)
+
+    def show_metadata_for_photo(self, photo_id: int, photo_path: str, metadata: dict = None):
+        """
+        Show the metadata editor dock for a specific photo.
+
+        Called from lightbox or grid when user wants to edit metadata.
+
+        Args:
+            photo_id: Database photo ID
+            photo_path: File path
+            metadata: Optional pre-loaded metadata dict
+        """
+        dock = getattr(self, "metadata_editor_dock", None)
+        if dock:
+            dock.load_photo(photo_id, photo_path, metadata)
+            if not dock.isVisible():
+                dock.show()
+
     def _init_embedding_status_indicator(self):
         """
         Initialize the embedding coverage status bar indicator.
@@ -3979,4 +4162,4 @@
                 self.statusBar().showMessage(tr('status_messages.backfill_complete'))                
                 os.remove(path)
         except Exception as e:
-            print(f"[Status] backfill poll failed: {e}")
+            print(f"[Status] backfill poll failed: {e}")
--- MemoryMate-PhotoFlow-Refactored-main/services/job_manager.py
+++ MemoryMate-PhotoFlow-Refactored-main/services/job_manager.py
@@ -51,6 +51,7 @@
 import time
 import uuid
 import json
+import functools
 from enum import IntEnum
 from dataclasses import dataclass, field
 from datetime import datetime
@@ -60,6 +61,7 @@
 from PySide6.QtCore import QObject, Signal, QThreadPool, QTimer, QRunnable, Slot
 
 from services.job_service import get_job_service, Job
+from repository.job_history_repository import JobHistoryRepository
 from logging_config import get_logger
 
 logger = get_logger(__name__)
@@ -141,6 +143,8 @@
     total: int = 0
     paused: bool = False
     cancel_requested: bool = False
+    # Stored (signal, slot) pairs for deterministic disconnect on cleanup
+    _connections: List[tuple] = field(default_factory=list)
 
 
 class JobManagerSignals(QObject):
@@ -252,6 +256,14 @@
         self._tracked_counter = 0
         self._tracked_cancel_callbacks: Dict[int, Callable] = {}
         self._tracked_descriptions: Dict[int, str] = {}
+
+        # Persistent job history (graceful fallback if DB init fails)
+        self._history_repo: Optional[JobHistoryRepository] = None
+        self._last_hist_progress_write: Dict[int, float] = {}
+        try:
+            self._history_repo = JobHistoryRepository()
+        except Exception as e:
+            logger.warning(f"[JobManager] Job history disabled (DB init failed): {e}")
 
         self._initialized = True
         logger.info(f"[JobManager] Initialized with max {self._max_workers} workers")
@@ -405,6 +417,13 @@
             self._tracked_descriptions.pop(job_id, None)
             if active:
                 logger.info(f"[JobManager] Canceled tracked job {job_id}")
+                if self._history_repo:
+                    try:
+                        self._history_repo.finish(
+                            job_id=str(job_id), status='canceled', canceled=1)
+                    except Exception:
+                        pass
+                self._last_hist_progress_write.pop(job_id, None)
                 self.signals.job_canceled.emit(job_id, active.job_type)
                 self.signals.active_jobs_changed.emit(len(self._active_jobs))
             return True
@@ -424,13 +443,28 @@
                 if hasattr(active.worker, 'cancel'):
                     active.worker.cancel()
 
+                # Disconnect worker signals
+                self._disconnect_worker(job_id, active)
+
                 logger.info(f"[JobManager] Canceled running job {job_id}")
+                if self._history_repo:
+                    try:
+                        self._history_repo.finish(
+                            job_id=str(job_id), status='canceled', canceled=1)
+                    except Exception:
+                        pass
                 self.signals.job_canceled.emit(job_id, active.job_type)
             else:
                 logger.info(f"[JobManager] Canceled queued job {job_id}")
                 # Get job type from database
                 job = self._job_service.get_job(job_id)
                 job_type = job.kind if job else 'unknown'
+                if self._history_repo:
+                    try:
+                        self._history_repo.finish(
+                            job_id=str(job_id), status='canceled', canceled=1)
+                    except Exception:
+                        pass
                 self.signals.job_canceled.emit(job_id, job_type)
 
         return True
@@ -531,6 +565,13 @@
             self._tracked_cancel_callbacks[job_id] = cancel_callback
         if description:
             self._tracked_descriptions[job_id] = description
+
+        if self._history_repo:
+            try:
+                self._history_repo.upsert_start(
+                    job_id=str(job_id), job_type=job_type, title=description)
+            except Exception as e:
+                logger.debug(f"[JobManager] Failed to persist job start {job_id}: {e}")
 
         logger.info(f"[JobManager] Registered tracked job {job_id}: {job_type} â€” {description}")
         self.signals.job_started.emit(job_id, job_type, total)
@@ -573,6 +614,19 @@
                 started_at=active.started_at,
             )
 
+        # Throttled persistence (at most once per second per job)
+        if self._history_repo:
+            try:
+                frac = float(current) / float(total) if total else 0.0
+                now = time.time()
+                last = self._last_hist_progress_write.get(job_id, 0.0)
+                if now - last >= 1.0 or (total and current >= total):
+                    self._history_repo.update_progress(
+                        job_id=str(job_id), progress=frac)
+                    self._last_hist_progress_write[job_id] = now
+            except Exception:
+                pass
+
     def report_log(self, job_id: int, message: str):
         """
         Emit a log line for any job (tracked or managed).
@@ -621,6 +675,19 @@
 
         stats_json = json.dumps(stats or {})
 
+        # Persist to history DB
+        if self._history_repo:
+            try:
+                if success:
+                    self._history_repo.finish(
+                        job_id=str(job_id), status='succeeded', result=stats or {})
+                else:
+                    self._history_repo.finish(
+                        job_id=str(job_id), status='failed', error=error or 'error')
+            except Exception:
+                pass
+        self._last_hist_progress_write.pop(job_id, None)
+
         if success:
             logger.info(f"[JobManager] Tracked job {job_id} ({active.job_type}) completed: {stats}")
             self.signals.job_completed.emit(job_id, active.job_type, True, stats_json)
@@ -636,6 +703,24 @@
     def get_job_description(self, job_id: int) -> str:
         """Return the human-readable description for a tracked job, or ``""``."""
         return self._tracked_descriptions.get(job_id, "")
+
+    def get_history(self, limit: int = 200):
+        """Return recent tracked-job runs from persistent history, most recent first."""
+        if not self._history_repo:
+            return []
+        try:
+            return self._history_repo.list_recent(limit=limit)
+        except Exception:
+            return []
+
+    def clear_history(self) -> None:
+        """Clear persisted job history."""
+        if not self._history_repo:
+            return
+        try:
+            self._history_repo.clear_all()
+        except Exception:
+            pass
 
     def notify_user_active(self):
         """
@@ -907,6 +992,15 @@
         # Start worker
         self._thread_pool.start(worker)
 
+        # Persist to job history
+        if self._history_repo:
+            try:
+                desc = job.kind.replace('_', ' ').title()
+                self._history_repo.upsert_start(
+                    job_id=str(job.job_id), job_type=job.kind, title=desc)
+            except Exception:
+                pass
+
         logger.info(f"[JobManager] Started job {job.job_id}: {job.kind}")
         self.signals.job_started.emit(job.job_id, job.kind, active.total)
         self.signals.active_jobs_changed.emit(len(self._active_jobs))
@@ -947,37 +1041,65 @@
         return None
 
     def _connect_worker_signals(self, job_id: int, worker: QRunnable):
-        """Connect worker signals to job manager handlers."""
-        if hasattr(worker, 'signals'):
-            signals = worker.signals
-
-            # Progress signal
-            if hasattr(signals, 'progress'):
-                signals.progress.connect(
-                    lambda cur, total, msg, jid=job_id: self._on_worker_progress(jid, cur, total, msg)
-                )
-
-            # Finished signal
-            if hasattr(signals, 'finished'):
-                # Handle different signal signatures
-                try:
-                    signals.finished.connect(
-                        lambda *args, jid=job_id: self._on_worker_finished(jid, True, args)
-                    )
-                except Exception:
-                    pass
-
-            # Error signal
-            if hasattr(signals, 'error'):
-                signals.error.connect(
-                    lambda *args, jid=job_id: self._on_worker_error(jid, args)
-                )
-
-            # Face detected signal (for partial results)
-            if hasattr(signals, 'face_detected'):
-                signals.face_detected.connect(
-                    lambda path, count, jid=job_id: self._on_face_detected(jid, path, count)
-                )
+        """Connect worker signals to job manager handlers.
+
+        Uses functools.partial instead of lambdas so slots are storable,
+        disconnectable, and don't accidentally capture mutable state.
+        All connections use Qt.QueuedConnection because workers fire
+        from the QThreadPool, and downstream code may touch UI state.
+        """
+        if not hasattr(worker, 'signals'):
+            return
+
+        signals = worker.signals
+        conn_type = Qt.ConnectionType.QueuedConnection
+
+        with self._jobs_lock:
+            active = self._active_jobs.get(job_id)
+        if not active:
+            return
+
+        def _store(sig, slot):
+            """Connect and remember (signal, slot) for deterministic disconnect."""
+            sig.connect(slot, conn_type)
+            active._connections.append((sig, slot))
+
+        # Progress signal
+        if hasattr(signals, 'progress'):
+            _store(signals.progress,
+                   functools.partial(self._on_worker_progress, job_id))
+
+        # Finished signal
+        if hasattr(signals, 'finished'):
+            _store(signals.finished,
+                   functools.partial(self._on_worker_finished_adapter, job_id))
+
+        # Error signal
+        if hasattr(signals, 'error'):
+            _store(signals.error,
+                   functools.partial(self._on_worker_error_adapter, job_id))
+
+        # Face detected signal (partial results)
+        if hasattr(signals, 'face_detected'):
+            _store(signals.face_detected,
+                   functools.partial(self._on_face_detected, job_id))
+
+    def _on_worker_finished_adapter(self, job_id: int, *args):
+        """Adapter for worker.signals.finished which may emit varying arg counts."""
+        self._on_worker_finished(job_id, True, args)
+
+    def _on_worker_error_adapter(self, job_id: int, *args):
+        """Adapter for worker.signals.error which may emit varying arg counts."""
+        self._on_worker_error(job_id, args)
+
+    def _disconnect_worker(self, job_id: int, active: 'ActiveJob'):
+        """Deterministically disconnect all worker signal connections for a job."""
+        for sig, slot in active._connections:
+            try:
+                sig.disconnect(slot)
+            except (RuntimeError, TypeError):
+                pass  # already disconnected or object deleted
+        active._connections.clear()
 
     def _on_worker_progress(self, job_id: int, current: int, total: int, message: str):
         """Handle worker progress update."""
@@ -1018,6 +1140,9 @@
             active = self._active_jobs.pop(job_id, None)
 
         if active:
+            # Deterministic disconnect before anything else
+            self._disconnect_worker(job_id, active)
+
             # Complete in database
             self._job_service.complete_job(job_id, success=success)
 
@@ -1030,6 +1155,15 @@
                     'total_count': args[2]
                 }
 
+            # Persist to job history (managed jobs, positive IDs)
+            if self._history_repo:
+                try:
+                    self._history_repo.finish(
+                        job_id=str(job_id), status='succeeded' if success else 'failed',
+                        result=stats if stats else None)
+                except Exception:
+                    pass
+
             logger.info(f"[JobManager] Job {job_id} completed: {stats}")
             self.signals.job_completed.emit(job_id, active.job_type, success, json.dumps(stats))
             self.signals.active_jobs_changed.emit(len(self._active_jobs))
@@ -1049,7 +1183,19 @@
             active = self._active_jobs.pop(job_id, None)
 
         if active:
+            # Deterministic disconnect before anything else
+            self._disconnect_worker(job_id, active)
+
             self._job_service.complete_job(job_id, success=False, error=error_msg)
+
+            # Persist to job history
+            if self._history_repo:
+                try:
+                    self._history_repo.finish(
+                        job_id=str(job_id), status='failed', error=error_msg)
+                except Exception:
+                    pass
+
             logger.error(f"[JobManager] Job {job_id} failed: {error_msg}")
             self.signals.job_failed.emit(job_id, active.job_type, error_msg)
             self.signals.active_jobs_changed.emit(len(self._active_jobs))
@@ -1112,6 +1258,32 @@
 _job_manager_lock = Lock()
 
 
+    def enable_startup_throttle(self, max_threads: int = 1) -> None:
+        """Temporarily reduce shared thread pool concurrency during startup.
+
+        This prevents background jobs (maintenance, embedding warmups, etc.) from competing
+        with initial layout stabilization and first paint.
+        """
+        try:
+            if not hasattr(self, "_startup_throttle_prev"):
+                self._startup_throttle_prev = self._thread_pool.maxThreadCount()
+            self._thread_pool.setMaxThreadCount(max(1, int(max_threads)))
+            logger.info(f"[JobManager] Startup throttle enabled (max_threads={max_threads})")
+        except Exception as e:
+            logger.warning(f"[JobManager] Failed to enable startup throttle: {e}")
+
+    def disable_startup_throttle(self) -> None:
+        """Restore shared thread pool concurrency after startup."""
+        try:
+            prev = getattr(self, "_startup_throttle_prev", None)
+            if prev is None:
+                return
+            self._thread_pool.setMaxThreadCount(int(prev))
+            delattr(self, "_startup_throttle_prev")
+            logger.info(f"[JobManager] Startup throttle disabled (restored max_threads={prev})")
+        except Exception as e:
+            logger.warning(f"[JobManager] Failed to disable startup throttle: {e}")
+
 def get_job_manager() -> JobManager:
     """
     Get the singleton JobManager instance.
@@ -1129,4 +1301,4 @@
     with _job_manager_lock:
         if _job_manager_instance is None:
             _job_manager_instance = JobManager()
-        return _job_manager_instance
+        return _job_manager_instance
