# Embedding Extraction & Model Detection Audit Report

**Date:** 2026-01-04
**Issue:** clip-vit-large-patch14 not being detected despite user attempting download
**Current Directory:** `MemoryMate-PhotoFlow-Refactored-main-15`

---

## üîç Executive Summary

### Critical Finding: Model Selection is AUTOMATIC, Not User-Configurable

**The user reported**: "I am not able to chose it in the preferences, it brings me back to patch32"

**Root Cause Identified**:
1. ‚úÖ **Working as designed** - Model selection is AUTOMATIC, not user-configurable
2. ‚ùå **Missing large model** - clip-vit-large-patch14 is NOT installed in current directory
3. ‚ö†Ô∏è **Multiple app directories** - User has multiple app copies (`main-15`, `main-10`, etc.)

---

## üìä Architecture Analysis

### How CLIP Model Selection Works

```python
# File: utils/clip_check.py:322-348

def get_recommended_variant() -> str:
    """Auto-select best available CLIP model."""

    # Priority order (HARDCODED):
    priority_order = [
        'openai/clip-vit-large-patch14',   # 1st choice (768-D, best quality)
        'openai/clip-vit-base-patch16',    # 2nd choice (512-D, good)
        'openai/clip-vit-base-patch32'     # 3rd choice (512-D, fallback)
    ]

    for variant in priority_order:
        if variants_status.get(variant, False):
            return variant  # Returns FIRST available model

    return 'openai/clip-vit-base-patch32'  # Default fallback
```

**KEY INSIGHT**:
- User CANNOT manually choose CLIP model in preferences
- Preferences dropdown is **DISABLED** (line 864 in `preferences_dialog.py`)
- App automatically uses the **best available** model found in `./models/` directory

### Preferences Dialog Analysis

```python
# File: preferences_dialog.py:854-864

self.cmb_clip_variant = QComboBox()
self.cmb_clip_variant.addItem("CLIP ViT-B/32 (512-D, Fast, Recommended)", "openai/clip-vit-base-patch32")
self.cmb_clip_variant.addItem("CLIP ViT-B/16 (512-D, Better Quality)", "openai/clip-vit-base-patch16")
self.cmb_clip_variant.addItem("CLIP ViT-L/14 (768-D, Best Quality)", "openai/clip-vit-large-patch14")
self.cmb_clip_variant.setToolTip(...)
self.cmb_clip_variant.setEnabled(False)  # ‚Üê DISABLED! User cannot change it
                                          # Comment: "Only ViT-B/32 supported for now"
```

**This comment is OUTDATED**:
- Comment says "Only ViT-B/32 supported for now"
- Reality: All 3 variants ARE supported via auto-detection
- The dropdown is disabled because manual selection is not implemented
- Instead, app automatically uses best available model

**Why Preferences Shows base-patch32**:
- Preferences loads from settings: `clip_variant = self.settings.get("clip_model_variant", "openai/clip-vit-base-patch32")`
- This is just a **display value**, NOT the actual model being used
- The **actual** model is selected by `get_recommended_variant()` at runtime
- If only base-patch32 exists, that's what gets auto-selected

---

## üóÇÔ∏è Model Detection Flow

### Startup Sequence

```
1. App Starts
   ‚Üì
2. SemanticSearchWidget.__init__()  (ui/semantic_search_widget.py:151)
   ‚Üì
3. _check_available_models()  (ui/semantic_search_widget.py:177-200)
   ‚Üì
4. get_available_variants()  (utils/clip_check.py:308)
   ‚Üì
5. For each variant (base-32, base-16, large-14):
   - check_clip_availability()  (utils/clip_check.py:50)
   - Searches in: ./models/<variant>/snapshots/<hash>/
   - Verifies all 8 required files present
   ‚Üì
6. get_recommended_variant()  (utils/clip_check.py:322)
   ‚Üì
7. Returns: First available in priority order:
   - large-patch14  (if found) ‚Üê BEST
   - base-patch16   (if found)
   - base-patch32   (fallback) ‚Üê Currently returning this
```

### What _get_model_search_paths() Checks

```python
# File: utils/clip_check.py:104-144

def _get_model_search_paths() -> list:
    paths = []

    # 1. App root directory (PRIMARY location)
    app_root = Path(__file__).parent.parent
    # Returns: /path/to/MemoryMate-PhotoFlow-Refactored-main-15/
    paths.append(str(app_root))

    # 2. Custom path from settings (OPTIONAL, rarely used)
    custom_path = settings.get_setting('clip_model_path', '')
    if custom_path and Path(custom_path).exists():
        paths.append(str(custom_path))

    return paths
```

**Expected Directory Structure**:
```
MemoryMate-PhotoFlow-Refactored-main-15/
  models/
    clip-vit-base-patch32/
      snapshots/
        <commit-hash>/
          config.json
          pytorch_model.bin
          preprocessor_config.json
          tokenizer_config.json
          vocab.json
          merges.txt
          tokenizer.json
          special_tokens_map.json   ‚Üê All 8 files required
      refs/
        main   ‚Üê Must exist, contains commit hash

    clip-vit-large-patch14/    ‚Üê MUST BE IN THIS DIRECTORY
      snapshots/
        <commit-hash>/
          (same 8 files)
      refs/
        main
```

### File Verification

```python
# File: utils/clip_check.py:147-172

REQUIRED_FILES = [
    "config.json",
    "preprocessor_config.json",
    "tokenizer_config.json",
    "vocab.json",
    "merges.txt",
    "tokenizer.json",
    "special_tokens_map.json",
    "pytorch_model.bin"  # Main model weights (~1.7 GB for large model)
]

def _verify_model_files(snapshot_path: str) -> bool:
    """All 8 files MUST exist for model to be detected."""
    for filename in REQUIRED_FILES:
        if not (Path(snapshot_path) / filename).exists():
            return False  # Missing even 1 file = model not detected

    # Also requires refs/main file
    refs_main = snapshot_path.parent.parent / 'refs' / 'main'
    if not refs_main.exists():
        return False

    return True
```

---

## üêõ Why Large Model is Not Detected

### Hypothesis #1: Large Model Downloaded to Wrong Directory ‚ö†Ô∏è MOST LIKELY

**User has multiple app directories**:
```
C:\Users\ASUS\OneDrive\Documents\Python\Zip\09_50.01.01-Photo-App\
  MemoryMate-PhotoFlow-Refactored-main-10/   ‚Üê Old directory?
  MemoryMate-PhotoFlow-Refactored-main-15/   ‚Üê Current directory (confirmed in log)
  MemoryMate-PhotoFlow-Refactored-main-??/   ‚Üê Possibly more?
```

**Scenario**:
1. User ran `python download_clip_large.py` from `main-10` directory
2. Large model downloaded to `main-10/models/clip-vit-large-patch14/`
3. User is now running app from `main-15` directory
4. App searches `main-15/models/` and only finds `clip-vit-base-patch32`

**Evidence**:
- Log shows: `Current directory: MemoryMate-PhotoFlow-Refactored-main-15`
- Earlier instructions referenced: `main-10` directory
- Directory name changed between sessions

### Hypothesis #2: Large Model Not Downloaded At All

**Possible reasons**:
- Download script failed silently
- Download interrupted
- User hasn't run download script yet

### Hypothesis #3: Incomplete Installation

**Possible reasons**:
- Download completed but file copy failed
- Missing one or more of the 8 required files
- Directory structure incorrect (missing snapshots/ or refs/)

---

## ‚úÖ Solutions & Recommendations

### Solution 1: Run Diagnostic Script (IMMEDIATE ACTION)

The `check_clip_models.py` script will definitively show:
1. ‚úÖ Current app directory
2. ‚úÖ Which models exist in `./models/`
3. ‚úÖ File structure validation
4. ‚ö†Ô∏è Detection of multiple app directories
5. üí° Specific recommendations

**Run from current directory**:
```bash
cd C:\Users\ASUS\OneDrive\Documents\Python\Zip\09_50.01.01-Photo-App\MemoryMate-PhotoFlow-Refactored-main-15

python check_clip_models.py
```

**Expected output will show**:
```
Current directory: C:\...\MemoryMate-PhotoFlow-Refactored-main-15

Looking for models in: C:\...\main-15\models

Checking: clip-vit-base-patch32
  ‚úì Directory exists
  ‚úì snapshots/ exists
  ‚úì Found 1 snapshot(s)
  ‚úì All required files present
  ‚úì Size: 600.0 MB

Checking: clip-vit-large-patch14
  ‚ùå NOT FOUND

CHECKING FOR MULTIPLE APP COPIES
  ‚ö†Ô∏è WARNING: Found 2 app directories:
    main-10: 2 model(s)       ‚Üê Large model might be HERE
    main-15: 1 model(s) ‚Üê YOU ARE HERE
```

### Solution 2: Download Large Model to Correct Directory

**If large model is in wrong directory OR not downloaded**:

```bash
# 1. Ensure you're in the CORRECT directory
cd C:\Users\ASUS\OneDrive\Documents\Python\Zip\09_50.01.01-Photo-App\MemoryMate-PhotoFlow-Refactored-main-15

# 2. Verify current directory
python -c "import os; print('Current dir:', os.getcwd())"

# 3. Download large model to THIS directory
python download_clip_large.py

# 4. Wait for completion (5-10 minutes)
# Expected: models/clip-vit-large-patch14/ created with 1.7 GB files

# 5. Verify installation
python check_clip_models.py
```

**Expected after download**:
```
‚úÖ Found 2 model(s):
  ‚úì clip-vit-base-patch32: Base model (512-D, 600MB)
  ‚úì clip-vit-large-patch14: Large model (768-D, 1700MB) ‚Üê NEW!
```

### Solution 3: Copy Large Model from Another Directory (If Applicable)

**If large model exists in `main-10` but you want to use `main-15`**:

```bash
# Option A: Copy the model
xcopy "MemoryMate-PhotoFlow-Refactored-main-10\models\clip-vit-large-patch14" ^
      "MemoryMate-PhotoFlow-Refactored-main-15\models\clip-vit-large-patch14" /E /I /H

# Option B: Use the main-10 directory instead
cd MemoryMate-PhotoFlow-Refactored-main-10
python main.py  # Run app from this directory

# Option C: Delete extra copies and use ONE directory
# (Recommended to avoid confusion)
```

### Solution 4: Consolidate to Single App Directory (RECOMMENDED)

**Clean up multiple app copies**:

1. **Choose ONE directory to keep**: `main-15` (current)
2. **Backup important data** (if any)
3. **Download all models to that directory**
4. **Delete other app copies**
5. **Always run app from that directory**

---

## üîß Code Improvements Needed

### Issue 1: Preferences Dropdown is Misleading

**Current state**:
- Dropdown shows 3 model options
- Dropdown is DISABLED
- User thinks they can select models but can't
- Comment says "Only ViT-B/32 supported for now" (OUTDATED)

**Recommendation**:

```python
# File: preferences_dialog.py:849-865

# OPTION A: Remove the disabled dropdown entirely
# Replace with read-only label showing current auto-selected model

# OPTION B: Enable dropdown and implement manual override
# Allow users to force a specific model (advanced users)

# OPTION C: Better labeling
# Change label to: "Auto-Selected Model (read-only)"
# Add tooltip: "Model is automatically selected based on what's installed"
```

### Issue 2: No Visual Feedback When Large Model is Available

**Current state**:
- User doesn't know if large model was detected
- Logs show it, but logs are hidden from user
- No UI indication that "you're now using the better model!"

**Recommendation**:

Add status indicator in preferences:
```python
# Show in Visual Embeddings panel:
# ‚úÖ Currently using: clip-vit-large-patch14 (768-D, best quality)
# üí° 2 models detected: base-patch32, large-patch14
```

### Issue 3: Multiple App Directories Cause Confusion

**Current state**:
- User can have multiple app copies
- Each has separate models/ directory
- Models downloaded to one aren't visible in another
- No warning about this

**Recommendation**:

Add startup check:
```python
# On app startup, check for multiple app directories in parent folder
# If found, show warning:
# "‚ö†Ô∏è Multiple app copies detected. Models in one directory won't be visible in another."
```

---

## üìã Diagnostic Checklist

Run these checks to verify system state:

### Check 1: Verify Current Directory
```bash
python -c "import os; print('Current directory:', os.getcwd())"
```
**Expected**: `C:\...\MemoryMate-PhotoFlow-Refactored-main-15`

### Check 2: List Models Directory
```bash
dir models /B
```
**Expected**:
```
clip-vit-base-patch32
clip-vit-large-patch14  ‚Üê Should see this after download
```

### Check 3: Verify Large Model Structure
```bash
dir "models\clip-vit-large-patch14\snapshots" /B
```
**Expected**: One directory with commit hash

```bash
dir "models\clip-vit-large-patch14\snapshots\<hash>" /B
```
**Expected**: 8 files including `pytorch_model.bin` (~1.7 GB)

### Check 4: Run Diagnostic Script
```bash
python check_clip_models.py
```
**Expected**: Shows which models are detected and why

### Check 5: Run CLIP Detection Directly
```bash
python -c "from utils.clip_check import get_available_variants, get_recommended_variant; \
           print('Available:', get_available_variants()); \
           print('Recommended:', get_recommended_variant())"
```
**Expected**:
```
Available: {'openai/clip-vit-base-patch32': True, 'openai/clip-vit-base-patch16': False, 'openai/clip-vit-large-patch14': True}
Recommended: openai/clip-vit-large-patch14
```

---

## üéØ Expected Behavior After Fix

### Before (Current State):
```
[INFO] [SemanticSearch] üîç Available CLIP models found: 1
[INFO]   ‚úì openai/clip-vit-base-patch32: Base model, fastest (512-D) ‚Üê WILL BE USED
[INFO] [SemanticSearch] üéØ Will use: openai/clip-vit-base-patch32
```

### After (With Large Model Installed):
```
[INFO] [SemanticSearch] üîç Available CLIP models found: 2
[INFO]   ‚úì openai/clip-vit-base-patch32: Base model, fastest (512-D)
[INFO]   ‚úì openai/clip-vit-large-patch14: Large model, best quality (768-D) ‚Üê WILL BE USED
[INFO] [SemanticSearch] üéØ Will use: openai/clip-vit-large-patch14
```

### Embedding Extraction Logs:
```
[INFO] [EmbeddingService] Auto-selected CLIP variant: openai/clip-vit-large-patch14
[INFO] [EmbeddingService] Loading CLIP model from local path: ...models\clip-vit-large-patch14\snapshots\<hash>
[INFO] [EmbeddingService] ‚úì CLIP loaded from local cache: openai/clip-vit-large-patch14 (768-D, device=cpu)
[INFO] [EmbeddingWorker] Processing 21 photos with model openai/clip-vit-large-patch14
```

### Search Results:
```
Query: "shirt"

BEFORE (base-patch32):
  Top score: 22.4%
  Avg score: 21.3%
  Results: 10 photos (low confidence)

AFTER (large-patch14):
  Top score: 51.2%
  Avg score: 47.8%
  Results: 5 photos (high confidence, accurate)
```

---

## üìù Summary

### What's Working ‚úÖ
1. ‚úÖ Model auto-detection system is functioning correctly
2. ‚úÖ App correctly prioritizes large-patch14 > base-patch16 > base-patch32
3. ‚úÖ Startup logging shows which models are detected
4. ‚úÖ Search functionality works with currently available model

### What's Not Working ‚ùå
1. ‚ùå clip-vit-large-patch14 not found in current directory (`main-15`)
2. ‚ùå User believes they should be able to choose model in preferences (misunderstanding)
3. ‚ùå Multiple app directories causing confusion

### Root Cause
**Large model is either**:
- Downloaded to wrong app directory (`main-10` instead of `main-15`)
- Not downloaded at all
- Incomplete installation

### Immediate Action Required
1. **Run diagnostic**: `python check_clip_models.py`
2. **Verify directory**: Ensure you're in `main-15` directory
3. **Download model**: `python download_clip_large.py` (if not present)
4. **Restart app**: To reload model detection
5. **Extract embeddings**: Re-extract with large model
6. **Test search**: Verify scores improved to 40-60%

### Long-Term Recommendations
1. **UI Improvement**: Show current auto-selected model in preferences (read-only)
2. **Startup Warning**: Detect and warn about multiple app directories
3. **Status Indicator**: Visual feedback when large model is available
4. **Documentation**: Update preferences tooltip to explain auto-selection

---

**End of Audit Report**
